# CS521 Ithemal Setup Guide

This project trains Ithemal on Bhive data set and evaluates the results. Code adpated from [Ithemal Repository](https://github.com/ithemal/Ithemal).

## Pipeline Overview

```
1. Docker Setup → 2. Preprocess Data → 3. Train Model → 4. Analyze Results
```

---

## 0. VM Setup (Optional)

Training requires significant CPU resources. Recommended AWS EC2 setup:

| Component | Recommendation |
|-----------|----------------|
| Instance | `c6a.8xlarge` (32 vCPU) or larger |
| AMI | Deep Learning Base AMI (Amazon Linux 2023) |
| Storage | 100GB EBS |

**Install docker-compose** (not included in the AMI):

```bash
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

## 1. Docker Setup

It is recommended to train the model in docker containers.

```bash
# Clone and build
git clone https://github.com/Aldo-Zhang/CS521_Ithemal.git
cd CS521_Ithemal/docker
sudo ./docker_build.sh        # ~10-15 min
sudo ./docker_connect.sh      # Start container

# Container management
sudo docker-compose down      # Stop
```

---

## 2. Data Preprocessing

Inside container at `/home/ithemal/ithemal`:

### Build Tokenizer (if not already built)

```bash
# Tokenizer should have been built with docker
# Verify tokenizer exists
ls data_collection/build/bin/tokenizer

# If not found, build it:
export ITHEMAL_HOME=/home/ithemal/ithemal
export DYNAMORIO_HOME=/home/ithemal/dynamorio/build
./build_all.sh
```

### Preprocess BHive Data

```bash
# Clone BHive dataset
git clone https://github.com/ithemal/bhive.git

# Run preprocessing (~5 min for all 3 architectures)
python bhive_preprocess.py

# Move to training directory
mv bhive_training_*.data learning/pytorch/inputs/data/
```

**Output**: `bhive_training_{ivb,hsw,skl}.data` (~300K samples each)

---

## 3. Model Training

### Training Command

```bash
# Train Ivy Bridge (replace ivb with hsw/skl for other architectures)
nohup python learning/pytorch/ithemal/run_ithemal.py \
    --data learning/pytorch/inputs/data/bhive_training_ivb.data \
    --use-rnn \
    train \
    --experiment-name bhive_ivb \
    --experiment-time $(date +%s) \
    --sgd \
    --threads 8 \
    --trainers 6 \
    --weird-lr \
    --decay-lr \
    --epochs 100 \
    > training_ivb.log 2>&1 &

# Sleep for 3 seconds
sleep 3
# Monitor
less +F training_ivb.log
```

### Recommended Settings

| vCPUs | threads | trainers | Estimated Time |
|-------|---------|----------|----------------|
| 32    | 8       | 6        | ~12 - 15 hours      |
| 96    | 14      | 8        | ~8 - 10 hours       |

### Output Location

```
learning/pytorch/saved/{experiment-name}/{timestamp}/
├── trained.mdl              # Model weights
├── predictor.dump           # Predictor for inference
├── loss_report.log          # Training loss per epoch
└── validation_results.txt   # Predicted vs actual (test set)
```

Once the training is done, it is recommended that to export the data from docker immediately

```
# 1. copy from Docker to EC2
docker cp ithemal:/home/ithemal/ithemal/learning/pytorch/saved ./saved

# 2. copy from EC2 to local
scp -r -i ~/.ssh/your-key.pem ec2-user@<EC2-IP>:/home/ec2-user/saved ./
```

---

## 4. Analyze Results

### Setup (local machine)

We assume all analysis are done on local machine, please structure results as follows:

### Expected Directory Structure

```
analysis/
├── analyze_results.py
├── results/
│   ├── ivb/
│   │   ├── validation_results.txt
│   │   └── loss_report.log
│   ├── hsw/
│   │   ├── validation_results.txt
│   │   └── loss_report.log
│   └── skl/
│       ├── validation_results.txt
│       └── loss_report.log
└── plots/                    # Generated by analyze_results.py
    ├── ivb_accuracy_heatmap.png
    ├── ivb_learning_curves.png
    ├── ...
```

### Setup Commands

```bash
# Install dependencies
pip install matplotlib seaborn pandas scipy numpy

# Run Analysis

cd analysis

python analyze_results.py --arch ivb --results_dir ./results/ivb/ --output_dir ./plots/
python analyze_results.py --arch hsw --results_dir ./results/hsw/ --output_dir ./plots/
python analyze_results.py --arch skl --results_dir ./results/skl/ --output_dir ./plots/
```

### Output

**Metrics**:
- MAPE, MAE, RMSE
- Pearson Correlation (all / <1000 cycles / <500 cycles)
- Spearman Rank Correlation
- Within 10%/20% error rate

**Plots** (saved to `analysis/plots/`):
| File | Description |
|------|-------------|
| `{arch}_accuracy_heatmap.png/pdf` | Prediction vs actual (≤1000 cycles) |
| `{arch}_accuracy_heatmap_full.png` | Full range heatmap |
| `{arch}_error_by_throughput.png/pdf` | Error by throughput range |
| `{arch}_learning_curves.png/pdf` | Training loss over epochs |
| `{arch}_distributions.png` | Prediction distribution |
| `{arch}_scatter.png` | Sampled scatter plot |

---

## Tests

New tests are architected for Bhive training pipeline specifically.

### BHive Pipeline Tests (Recommended)

```bash
# One-click test all BHive pipeline components
pytest bhive_tests/ -v

# Individual test modules
pytest bhive_tests/test_environment.py -v     # Environment & dependencies
pytest bhive_tests/test_data_loading.py -v    # Data loading
pytest bhive_tests/test_training_pipeline.py -v # Training pipeline (slow)
pytest bhive_tests/test_predict_pipeline.py -v  # Prediction pipeline (slow)
```
---

## Resources

- [Ithemal Paper (ICML 2019)](https://arxiv.org/abs/1808.07412)
- [Ithemal Repository](https://github.com/ithemal/Ithemal)
- [BHive Dataset](https://github.com/ithemal/bhive)
